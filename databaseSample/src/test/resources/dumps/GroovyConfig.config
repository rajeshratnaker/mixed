set verbosity QUIET

// Default library imports
import org.joda.time.*

// Default Coherence imports
import com.tangosol.util.*
import com.tangosol.util.aggregator.*
import com.tangosol.util.filter.*
import com.tangosol.util.extractor.*
import com.tangosol.util.comparator.*
import com.tangosol.net.*

// Default project imports
import com.oracle.coherence.common.identifiers.*
import com.rajinfra.common.*
import com.rajinfra.identifier.*
import com.rajinfra.attributedobject.*
import com.rajinfra.filter.*
import com.rajinfra.evictor.*
import com.rajmodel.ABC.*
import com.rajmodel.trader.*
import com.rajdao.common.*
import com.rajdao.coherence.aggregation.*

//
// Identifier support for wrapped types
//
Integer.metaClass.getId = { IntegerBasedIdentifier.newInstance(delegate) }
Long.metaClass.getId = { LongBasedIdentifier.newInstance(delegate) }
String.metaClass.getId = { StringBasedIdentifier.newInstance(delegate) }
ArrayList.metaClass.getId = { CompoundIdentifier.newInstance(delegate) }
 
//
// Useful function objects
//
shutdown = { CacheFactory.shutdown() }
cache = { name -> CacheFactory.getCache(name) }
nav = { obj, path -> new com.rajdao.coherence.CacheNavigableAttributedObject(obj).get(path) }
members = { applicationService.getClusterMemberData() }
keyOwner = { cacheName, key -> CacheFactory.getCache(cacheName).getCacheService().getKeyOwner(key) }
invokeAll = { str -> clusterInvocationService.evaluate(str) }
gc = { clusterInvocationService.clusterWideGC() }
suspend = { applicationService.suspendApplication() }
resume = { applicationService.resumeApplication() }

setSaveApiBatchSize = { batchSize ->
        invokeAll("applicationContext.getBean('saveApiABCSubmissionSessionFactory').setBatchSize(${batchSize})")
}

runStatus = { 
    cache("ABCSubmission").values().each {
        println it.getABCSubmissionId()+"\t"+it.getBusinessDate()+"\t"+it.getSource()+"\t"+it.getFileDisplayName()+"\t"+it.getStateName()+"\t"+it.getLoadingProgress()+"\t"+it.getProgress() 
    }
    "Done."
}

clusterTotalMemoryInBytes = { applicationService.getClusterMemberData().collect{ it.getTotalMemory() }.sum() }
clusterFreeMemoryInBytes = { applicationService.getClusterMemberData().collect{ it.getFreeMemory() }.sum() }

clusterTotalMemoryInGB = { clusterTotalMemoryInBytes() / (1024**3) }
clusterFreeMemoryInGB = { clusterFreeMemoryInBytes() / (1024**3) }

status = {
	def freeHeap = clusterFreeMemoryInGB()
    def usedHeap = clusterTotalMemoryInGB() - freeHeap
    def members = members()
    def storageMembers = CacheFactory.getCache("Exposure").getCacheService().getStorageEnabledMembers()
    println "---- Cluster ----"
    printf "Storage members: %d\n", storageMembers.size()
    printf "Client members: %d\n", members.size() - storageMembers.size()
    printf "Total used heap: %,.2f GB\n", usedHeap.doubleValue()
    printf "Total free heap: %,.2f GB\n", freeHeap.doubleValue()
    println "Suspended: "+ applicationService.isClusterSuspended()
    println "---- ABC Submissions ----"
	
	grpAgg = GroupAggregator.Parallel.createInstance(ABCSubmissionDao.stateNameExtractor,new Count())
	cache("ABCSubmission").aggregate(AlwaysFilter.INSTANCE, grpAgg).each{key, value -> println ("${key} :${value}")}

    println "---- ABC Summary ----"
    printf "Exposures: %,d\n", cache("Exposure").size()
    printf "Trades: %,d\n", cache("Trade").size()
    "Done."
}

processedCount = {
    cache("ABCSubmission").keySet(new OrFilter(new EqualsFilter("getStateName","Processed"), new EqualsFilter("getStateName","ProcessedWithErrors"))).size()
}

processingCount = {
    cache("ABCSubmission").keySet(new EqualsFilter("getStateName","Processing")).size()
}

processingTimes = {
    def keys = cache("dataflowEvent").entrySet(new OrFilter(new EqualsFilter("getEffect","Processed"), new EqualsFilter("getEffect","ProcessedWithErrors")))
    def elapsedTime = 0
    def count = 0
        keys.collect { it.getValue() }.sort { it.getCreatedAtTimestamp() }.each {
        elapsedTime += printSubmissionsForEvents( it )
        count++
    }
    println "Mean processing time: "+elapsedTime / count
    "Done."
}

loadingTimes = {
    def keys = cache("dataflowEvent").entrySet(new EqualsFilter("getEffect","Processing"))
    def elapsedTime = 0
    def count = 0
    keys.collect { it.getValue() }.sort { it.getCreatedAtTimestamp() }.each {
        elapsedTime += printSubmissionsForEvents( it )
        count++
    }
    println "Mean loading time: "+elapsedTime / count
    "Done."
}

time = { f ->
    def before = System.currentTimeMillis()
    f()
    String.format("%,d ms", System.currentTimeMillis() - before)
}

printSubmissionsForEvents = { event ->
        def dataflow = cache("dataflowEntity").get(event.getdataflowEntityId())
        def rs = cache("ABCSubmission").get(dataflow.getContext().getAssociatedId())
        def processedAt = new DateTime(event.getProcessedAtTimestamp())
        def elapsedTime = event.getProcessedAtTimestamp() - (rs.getCreationTime()==null?event.getCreatedAtTimestamp() : rs.getCreationTime().getMillis());
        println rs.getABCSubmissionId()+"\t"+rs.getBusinessDate()+"\t"+rs.getSource()+"\t"+rs.getFileDisplayName()+"\t"+String.format("%,d",elapsedTime)+" ms \t"+rs.getCreationTime()+"\t"+processedAt
        return elapsedTime
}

clearDownSystem = {
    cache("ABCSubmission").clear()
    cache("Exposure").clear()
    cache("Trade").clear()
    cache("dataflowEvent").clear()
    cache("dataflowEntity").clear()
    cache("Task").clear()
	cache("ExportTask").clear()
    "Done."
}

isLive = { dataflow ->
                        units = cache("ExportUnits").values().collect { it.get("unitId") }
                        id = dataflow.context.associatedId
                        if(id!=null) {
                            return units.contains( traderCodeDao.get( id  )?.unitId?.integer )
                        }
                        return false
}

signOffBooks = {
        canSignOff = { dataflow -> dataflow.isEventValidForCurrentNode("SignedOff") }
        signOff = { dataflow -> dataflowService.fireEvent("SignedOff", dataflow) }
        dataflowService.getdataflowEntities("bookdataflow").findAll { canSignOff(it) && isLive(it) }.each { signOff(it) }
}

ackErrorsForBooks = {  
        canAck = { dataflow -> dataflow.isEventValidForCurrentNode("Error") }
        ack = { dataflow -> dataflowService.fireEvent("AcknowledgeErrors", dataflow) }
        dataflowService.getdataflowEntities("bookdataflow").findAll { canAck(it) && isLive(it) }.each { ack(it) }
}

testExportStatus = { out,cobDate,content ->
         units = cache("ExportUnits").values().collect { it.get("unitId") }
         units.each {
                result = dataflowService.getdataflowEntities("unitdataflow", IntegerBasedIdentifier.newInstance(it)).collect{it};
		  if(content.contains(""+it)){

                if(result.isEmpty()) {
                        printFilesForUnit(it,cobDate, out);
                        //out.write ("Unit "+it+" not arrived, waiting for ", it.getExpectedImportFiles() )
                } else {
                        result.findAll{it.currentNode.name!="Exported" && it.currentNode.name!="ExportError" }.each{
			   if(it.context.getCloseOfBusiness().toString().equals(cobDate)){
                        out.write (it.context.getCloseOfBusiness().toString()+","+it.context.getAssociatedId()+","+it.context.getReportStatus()+","+it.currentNode.getName());
                        out.write ("\n")   
        		unit = unitDao.get(it.context.getAssociatedId());
                        booksNotArrived = (unit.expectedTraderBooks.minus(it.context['disabledBooks'])).minus(it.context['completedBooks']);
						out.write("Expected files:\n");
						booksNotArrived.each {
							it.importFiles.each{ out.write("\t"+it.fileNameFormat+"\n");}
						}
 			  }	
			  }
                }

		  }
		 
         }==null
}


printFilesForUnit = {unitId, date, out  -> 
	unit = unitDao.get(Unit.newIdentifier(unitId));
	
	out.write(date.toString() + " Unit " + unit.id + " not arrived, waiting for\n");
	if (unit != null && !unit.expectedImportFiles.isEmpty()) {
		unit.expectedImportFiles.each{ out.write("\t"+it.fileNameFormat+"\n");}==null
	}
	
}        


testExportStatusToFile = { fileName,cobDate,prodUnitsFileName ->
	content = new File(prodUnitsFileName).text; 
	new File(fileName).withWriter { out -> testExportStatus(out,cobDate,content) } }
	
testExportStatusToConsole = {cobDate,prodUnitsFileName->
	content = new File(prodUnitsFileName).text; 
	testExportStatus( System.out,cobDate,content) 
	}

exportUnits = { date, units ->
         units.each{ ABCSubmissionService.exportUnit(Unit.newIdentifier(it), date, "O", "ABC") }
}

exportTestUnit = {date, unitId, testUnitId ->
        unit = unitDao.get(Unit.newIdentifier(unitId));
        testUnit = unitDao.get(Unit.newIdentifier(testUnitId));
        exposureService.export(date,unit,testUnit,"O","ABC");
        }
        
// Remove ABC submission, dataflow and exposures. Trades will be overwritten
// Example use:  cancelABCRun( 1234L.id )
cancelABCRun = { id ->
    def ABCSub = cache("ABCSubmission").get(id)
    def dataflow = dataflowService.getdataflowEntity("ABCSubmissiondataflow", id, ABCSub.getBusinessDate(), ABCSub.getReportStatus(), ABCSub.getContent())
    cache("ABCSubmission").remove(id)
    cache("dataflowEntity").remove(dataflow.getId())
    def eventIds = cache("dataflowEvent").keySet(new EqualsFilter("getdataflowEntityId",dataflow.getId()))
    eventIds.each { cache("dataflowEvent").remove(it) }
    exposureDao.removeExposuresForABCSubmission(id)
    "Done."
}

distribution = { cacheName ->
	CacheFactory.ensureCluster().getMemberSet().collect { 
		[it.getId(), cache(cacheName).keySet(new LocalFilter(it,AlwaysFilter.INSTANCE)).size()] }.findAll{ it[1] > 0 }
}

distributionSummary = {
		println(["Cache", "Mean", "StdDev", "Min", "Max"])
        elements = CacheFactory.getConfigurableCacheFactory().getConfig()
        .findElement("caching-scheme-mapping").getElements("cache-mapping");

        elements.each {
            cacheName = it.getElement("cache-name").getValue();
            if(cache(cacheName).size()>1000) {
            	values = distribution(cacheName).collect{ it[1] }
            	mean = values.sum() / values.size()
            	stdDev = Math.sqrt( values.collect{(it-mean)**2}.sum() )
            	println([cacheName, mean, stdDev, values.min(), values.max() ])
            }
        }
    }


loadEvictionPolicies = { xmlPath -> new org.springframework.context.support.FileSystemXmlApplicationContext(xmlPath).getBeansOfType(EvictionPolicy.class).values() }

evictors = { rules -> rules.collect{
        if (ContainerType.CACHE.equals(it.getContainerType())) {
            return new FilteredCacheEvictor(springAwareCacheFactory,it)
        } else if (ContainerType.FILESYSTEM.equals(it.getContainerType())) {
            return new FileSystemEvictor(it)
        } else {
            throw new IllegalArgumentException()
        }
    }
}       

checkClusterMember = {println("===============================================");
		      println("===============================================");
                      clusterMembers = clusterInvocationService.getClusterMemberData(); 
		      println("Number of Cluster Members "+clusterMembers.size());
		      println("===============================================");
		      clusterMembers.each{
		      memberData -> println("Address is "+memberData.getMember().getAddress()+" ,port "+memberData.getMember().getPort()+" ,machine name is "+memberData.getMember().getMachineName()+" ,process name is "+memberData.getMember().getProcessName()+"  " + (memberData.isSuspended() ? "suspended" : "notSuspended"))
		      }
		      println("===============================================");
		      }

//////////////////////////////////////////
println "project Console Configuration Loaded"
set verbosity INFO
